# TODO: 3x3 (5x5) dw (linear) + 1x1 intermediate (linear) // 4 + 1x1 (ReLU)

[net]
# Testing
#batch=1
#subdivisions=1
# Training
batch=64
subdivisions=8
width=960
#height=540
height=512
channels=3
momentum=0.9
decay=0.0005
angle=0
saturation = 1.5
exposure = 1.5
hue=.1

learning_rate=0.001
burn_in=500
max_batches = 60000
policy=steps
steps=30000, 45000
scales=0.1, 0.1

[convolutional]
batch_normalize=1
filters=32
size=7
stride=4
pad=3
activation=relu

####

[convolutional]
batch_normalize=1
groups=32
filters=32
size=3
stride=1
pad=1
activation=linear
[convolutional]
batch_normalize=1
filters=32
size=1
stride=1
pad=1
activation=relu

[convolutional]
batch_normalize=1
groups=32
filters=32
size=3
stride=2
pad=1
activation=linear
[convolutional]
batch_normalize=1
filters=64
size=1
stride=1
pad=1
activation=relu

####

[convolutional]
batch_normalize=1
groups=64
filters=64
size=3
stride=1
pad=1
activation=linear
[convolutional]
batch_normalize=1
filters=64
size=1
stride=1
pad=1
activation=relu
[shortcut]
from=-3
activation=linear

[convolutional]
batch_normalize=1
groups=64
filters=64
size=3
stride=2
pad=1
activation=linear
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=relu

####

[convolutional]
batch_normalize=1
groups=128
filters=128
size=3
stride=1
pad=1
activation=linear
[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=relu
[shortcut]
from=-3
activation=linear

[convolutional]
batch_normalize=1
groups=128
filters=128
size=3
stride=2
pad=1
activation=linear
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=relu

####

[convolutional]
batch_normalize=1
groups=256
filters=256
size=3
stride=1
pad=1
activation=linear
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=relu
[shortcut]
from=-3
activation=linear

[convolutional]
batch_normalize=1
groups=256
filters=256
size=3
stride=1
pad=1
activation=linear
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=relu
[shortcut]
from=-3
activation=linear

[convolutional]
batch_normalize=1
groups=256
filters=256
size=3
stride=1
pad=1
activation=linear
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=relu
[shortcut]
from=-3
activation=linear

[convolutional]
batch_normalize=1
groups=256
filters=256
size=3
stride=1
pad=1
activation=linear
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=relu
[shortcut]
from=-3
activation=linear

[convolutional]
batch_normalize=1
groups=256
filters=256
size=3
stride=1
pad=1
activation=linear
[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=relu
[shortcut]
from=-3
activation=linear

####

[convolutional]
batch_normalize=1
groups=256
filters=256
size=3
stride=1
pad=1
activation=linear
[convolutional]
batch_normalize=1
filters=512
size=1
stride=1
pad=1
activation=relu

[convolutional]
batch_normalize=1
groups=512
filters=512
size=3
stride=1
pad=1
activation=linear
[convolutional]
batch_normalize=1
filters=512
size=1
stride=1
pad=1
activation=relu

[convolutional]
size=1
stride=1
pad=1
filters=80
activation=linear


[region]
anchors =  0.298,0.256, 0.602,0.566, 0.932,0.772, 1.225,1.190, 1.858,1.681, 1.895,0.854, 2.713,2.490, 3.381,1.460, 4.488,3.066, 7.450,5.975
bias_match=1
classes=3
coords=4
num=10
softmax=1
jitter=.2
rescore=1

object_scale=5
noobject_scale=1
class_scale=1
coord_scale=1

absolute=0
thresh=0.5
random=0
